---
title: "Statistical Analysis of Insurance Coverage Against Social Status"
author: "Kevil Khadka, Jialin Xiang, Rafael Pereira"
date: "11/29/2019"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(GGally)
library(tidyverse)
library(Sleuth3)
library(MASS)
library(leaps)
library(car)
```

## Abstract

This study works on a dataset which includes the information about the insurance companies which were denying the insurance coverage for the certain number of people (miniority) living within geographical areas. This project focuses on relationship between the minority and denial of coverage. More specifically,we look certain explanatory variables to find out if there are any reasons why people living within certain areas have to take governemnt issued insurance policy instead of private insurance. The purpose of the analysis is to find out the ethical reason to see if there was any discriminiation upon minority people from private insurance companies. We build a mulitple linear regression model that helps to explain if the variables like fires, thefts and minor have any influence on the being denied from the insurance companies.
We found that there is pretty good linear relationship between the percentage of population as minority and government issued policies. Also we discovered that the private companies were discriminiating the minority people by denying their insurance policies.

## Introduction

Do the private insurance companies look at all people's applications equally when deciding whether to provide coverage? Are they illegaly canceling the applications based on the applicant's background and history of their residence area?

Nearly a decade after the black civil righs movement in the United States (1954 - 1968), investigations were conducted to look into reports of health care denials to certain people. The information collected after the commission's inspections resulted in 47 observations of different geographical areas of Chicago, Illinois, which provided some interesting insights. Based on the zipcode provided on the dataset, it was found that miniority people were residing in Chicago area. Today, 50 years later, after some data analysis and some data cleaning, multiple linear regression models can be created to demonsrate that discrimination did exist against minorities in the late 1970s. It seems that insurance companies were actually denying coverage for certain people. On this data analysis report, we want to create a simple model that could answer our questions of interest, which are listed below: 

(1) Is there evidence of a relationship between race and the denial of coverage?
(2) Can that relationship be explain away by the insurance companies? 
(3) Did the insurance companies illegally discriminate against minorities?

This study will look the most accuarate model to answer our questions by using various data anaylsis method, checking any influential outliers, refining the model, and selection techniques.

## Dataset  

The data set we used to study our question had eight variables (two categorical, and six quantitative), and 47 rows of data representing geographical areas of the state of Illinois. It provided the number of government-offered policies that were issued broken down by ZIP Code. Those people who were denied by private insurance coverage received the government-issued policies provided by the government.Though, to build the best model and make a good prediction for our question, we might have to fix some of our data. The reason for this is the presence of aggregated points in our observations. 

These are the eight variables we worked with:

Zip : ZIP Code of area;

Minor : Percentage of population as minority;

Fires : Number of fires per 100 houses;

Thefts : Number of thefts per 1000 people;

Const : Percentage of houses constructed before 1939;

Plan : New government-issued policies and renewals per 100 houses;

Income : Median income for family (thousands of dollars);

Location : Location of house (north or south side of city).

## Exploratory Data Analysis
We started our project by looking the summary of dataset. We found out there was a quite wide range in the minor variable based on zipcode of area where residence were living. The dataset has the data of people living in north and south location area of chicago. The area seems quite damage from fires inciden and large number of thefts record. Not quite sure if the provided data about income is per monthly but people were having decent life.
Since our question of interest is to look after the relationship between minority and denied coverage, we thought looking at the scatterplot matrix to find out how other variables interact with reposne variable, plan. 

*Scatterplot Matrix*
```{r, include=FALSE}
project <- read_csv("civil_rights.csv")
summary(project)
```

```{r, echo=FALSE}
ggpairs(data=project,columns = c(2:7), progress = FALSE)
```
Analyzing the scatterplot matrix on multiple explantory variables, we found some good linear relationship between our variables. After that we built the scatterplot of each explanatory variables with plan, and looked at their summary model. We checked if transformation of variables is necessary and found out that it was not quite good to do so.

*Scatterplot of minor vs plan*
```{r, echo = FALSE}
ggplot(mapping = aes(x = minor, y = plan), data = project) +
  geom_point(aes(color = location)) + 
  geom_smooth(method = "lm") +
  labs(x = "Percentage of Minority", y = "Government-Issued Plans")
```

*Summary of model of plan vs minor*
```{r}
fit <- lm(plan ~ minor, data = project)
summary(fit)
```

*Diagnostic plot*
```{r}
plot(fit, which =1)
```
As mentioned earlier, there is a strong linear relationship between the percentage of minority and the government-issued insurance plans. Based on a high correlation of 71.4% seen at the scatterplot matrix, we decided to not to perform transformation for this relationship. The bigger the percentage of minorities living in the region, the more government-issued insurance plans we observe. From those observations, we fit a simple linear regression model to see the $R^2$ between the insurance plan and the percentage of minority, which is $0.5094$. The plot of residuals vs. fitted value shows that our residuals are normally distributed, but we have a problem of nonconstant variance of residuals.
We found out the p-value is less than 0.05 which provide the good evidence that there is a relationship between race and denial of coverage. 


After demonstrating a linear relationship between minor and plan, we now trying to fit a rich model to see if fire, thefts, const, income and location could influence government-issued coverage. Basically, we need to see if there was any ethical dilemma by not including the minorirty in our model. 
So our first model consists of all explanatory variable except minor to explain the mean response of plan. 
```{r}
new_model <- lm(plan ~ fires + thefts + const + income + location, data = project)
summary(new_model)
```
The following is our regression equation of the full model:
${Plan} = 0.084843 + {0.046870}*{(fires)} - {0.008677}*{(thefts)} + {0.007355}*{(const)} - {0.029004}*{(income)} + {0.217654}*(locations)$

Except income and locations, all explanatory variables seems statistically significant. Our full model explained $0.6802$ proportion of variation using all variables except minor.  
We check the residual vs fitted value plot of full. The residual plot and the histogram of residuals shows that our residuals are normally distributed, but we have a problem of nonconstant variance of residuals. 
```{r}
ggplot(new_model) + geom_point(aes(x = new_model$fitted.values, y = new_model$residuals)) + geom_hline(yintercept = mean(new_model$residuals)) +  labs(x = "Fitted Values", y = "Residuals", color="Location") + ggtitle("Residual vs Fitted Value")

plot(new_model, which = 1)
hist(new_model$residuals)
```

## Refining the Model 
We want to refine our model so we decided to look for outliers that could influence our regression model. If those outliers are present in our observations, we look forward to removing them in order to have a more accurate regression. We mutate data of leverage, studentized residuals and cook's distance into our dataset to check for influential outliers and plot those numerical values.

```{r, echo = FALSE}
project_new <- project %>% 
  mutate(leverage = hatvalues(new_model),
         studres = studres(new_model),
         cooks = cooks.distance(new_model))
```
*Plot of Leverage vs Zip*
```{r}
ggplot(data = project_new) + 
  geom_point(mapping=aes(x = zip, y = leverage)) + 
  ggtitle("Leverage vs. Zip Code")+ xlab("Zip Code") + ylab("Leverage")+
  ggrepel::geom_text_repel(aes(y=leverage, x=zip, label=zip),
                           data = project_new %>% 
                             filter(leverage>0.4|studres>2|studres< -2|cooks> 0.3))
```
Our first numerical measure is leverage and it will assess the distance between the values of the outlier's explanatory variable and the average of those values. The level of influence an outlier has on a regression model is measured by how large the leverage is. From the plot we constructed above, ZIP Codes $60607$ and $60611$ have large leverage.


*Plot of Studentized Residual vs Zip*
```{r}
ggplot(data = project_new) +
  geom_point(mapping=aes(x = zip, y=studres)) +
  ggtitle("Studentized Residuals vs. Zip Code")+ xlab("Zip Code") + ylab("Studentized Residuals")+
  ggrepel::geom_text_repel(aes(y=studres, x=zip, label=zip),
                           data = project_new %>% 
                             filter(leverage>0.4|studres>2|studres< -2|cooks> 0.3))
```
Secondly, the studentized residuals plot above illustrates how many standard deviations a residual is away from zero. The residual values are expected to be symetrically centered around zero. Zip Codes $60621$, $60624$ and $60610$ as potential outliers that could influence our regression.

*Plot of Cook's Distance vs Zip*
```{r}
ggplot(data = project_new) +
  geom_point(mapping=aes(x = zip, y= cooks)) +
  ggtitle("Cook's Distance vs. Zip Code")+ xlab("Zip Code") + ylab("Cook's Distance")+
  ggrepel::geom_text_repel(aes(y=cooks, x=zip, label=zip),
                           data = project_new %>%
                             filter(leverage>0.4|studres>2|studres< -2|cooks> 0.3))
```
A Cook's distance greater than one generally means that those outliers are influential and should be removed from the model. After calculating the Cook's distance, we can check for the effect of omitting those points which would have on the coefficients of our regression equation. From the graph above, we notice that the geographical regions of Zip Codes $60610$ and $60611$ appear again far away from the other points. A large Cook's distance is associated with an influential outlier, and taking that outlier out will cause non-influenced result in our regression. 

Zip Code $60607$ has a high leverage value but seems to show a low cook's distance and a studentized residual centered around zero, so we will not consider that data point as an outlier that could influence our multiple regression.

Based on the above plots we believe that the geographical areas with ZIP Code of $60610$ and $60611$ are influential outliers to our analysis. 
First we observed the outliers, and see what kind of data points they are. After observing the outliers, we planned to remove those influential outliers from our dataset. 

After we remove those two outliers, we take a look at the new model without those data points. Based on the summary of this second model without the outliers, the adjusted $R^2$ is $0.7165$, almost 8% higher accuracy than our first model that contained those two regions of Illinois in the data set.We also plotted the residual against the fitted values of our new model and we can still know that we have a problem with a nonconstant variance of residuals.
```{r, echo = FALSE}
project_new %>% filter(zip == '60610' | zip == '60611')
project_new_2 <- project_new %>% filter(zip != "60610" & zip != "60611")
print(project_new_2)
```

```{r}
new_model_2 <- lm(plan ~ fires + thefts + const + income + location, data = project_new_2)
summary(new_model_2)
```
*plot of Residual vs Fitted value of second model w/o outliers*
```{r}
ggplot(new_model_2) + geom_point(aes(x = new_model_2$fitted.values, y = new_model_2$residuals)) + geom_hline(yintercept = mean(new_model_2$residuals)) +  labs(x = "Fitted Values", y = "Residuals", color="Location") + ggtitle("Residual vs Fitted Value")

plot(new_model_2, which = 1)
```

Still, there are some insignificant explanatory vairables in our model. So, we run the Cp plot to get our reduced model. We even applied the variable selection technique to get our reduced model where fires, thefts and income seems best explanatory variables. Also, the BIC plot suggested same explanatory variables. 

*BIC plot*
```{r}
models_1 <- regsubsets(plan ~ fires+const+thefts+income+location, nbest = 2, data = project_new_2)
plot(models_1)
#We see that the same model as above was chosen as the best fitting model.
```

Our reduced model is given by: 
${(Plan|fires, thefts, income)} = 1.420636 + {0.048668}*{(fires)} - {0.008935}*{(thefts)} - {0.103400}*{(income)}$


This reduced model actually gives us a better adjusted R-squared value of 71.91% which means this reduced model able to explain more of the variability within the model. Since such a large variation in the model,We think private companies didnot show any interest on providing insurance policies to those minor people.
```{r}
# based on above graph and method, our reduced model is
reduced_model <- lm(plan ~ fires + thefts + income, data = project_new_2)
summary(reduced_model)
```

Since, our scatterplot matrix shows the constant relationship between thefts vs plan. So, we plot the partial residual plot to check the relationship where it shows good linear line between thefts vs plan.
*checking partial residual plots*
```{r}
crPlots(reduced_model)
```

Since we have our reduced model which shows total variation of variables within the model. We add minor variable on our reduced model to see if private insurance companies did discriminate on minor poeple or not. We run the anova table on reduced model and complete model (adding minor on reduced model) and see the p-value to decide which model to keep. 
```{r}
# NOW add minor on our reduced model say it says full model
complete_model <- lm(plan ~ minor + fires + thefts + income, data = project_new_2)
summary(complete_model)
plot(complete_model$residuals)
hist(complete_model$residuals)
```

```{r}
anova(reduced_model, complete_model)
```
From anova table, it gives a fairly low p-value (0.02227) indicating that the model with the minor variable explained a little more. Adjusted R-squared value is $0.7477$ which means that only about 74.77% of the total variation can be explained in the model. So, we planned to go with our complete model with minor variable. And we also conclude that there was discrimination on minorities people by private insurance companies.

```{r}
final_model <- lm(plan ~ minor + fires + thefts, data = project_new_2)
summary(final_model)

ggplot(final_model) + geom_point(aes(x = final_model$fitted.values, y = final_model$residuals)) + geom_hline(yintercept = mean(final_model$residuals)) +  labs(x = "Fitted Values", y = "Residuals", color="Location") + ggtitle("Residual vs Fitted Value of Final Model")

plot(final_model, which =  1)
hist(final_model$residuals)

```
Since, our complete model with minor has all siginificant p-value except income, we run the Cp plot on it to get better reduced and simple model. And applying same process of forward selection and backward elmination technique, we got the final model for our project where we decide to keep minor, fires and thefts as our explanatory vairables. 

We even did anova test on complete_model_2 ($plan = minor + fires + thefts$) and complete_model ($plan = minor + fires + thefts + income$).
```{r}
anova(complete_model_2, complete_model)
```
From anova table, p-value for complete_model($plan = minor + fires + thefts + income$) is more than 0.05, so we think it best to with model without income explanatory variable. Adjusted R-squared value is $0.7416$ and mulitple R-squared is $76.34$ which means that only about $76.34$% of the total variation can be explained in the model. 

Our Final regression model is given by 
${(Plan|minor, fires, thefts)} = 0.030825 + {0.007168}*{(minor)} + {0.051123}*{(fires)} - {0.008236}*{(thefts)}$

## Conclusion

At conclusion, we can say that there is good relationship between the race and the government-issued polices. After conducting the complete analysis of the dataset, our final model "complete_model_2" in which response variable plan and predictor vairable Minor, fires and thefts is better in predicting that the discrimination upon minority people by private insurance companies. Our final model, predicator variables explained about 76.34% variablitiy in response variable. Adding minor variable in our dataset was to conclude that the insurance companies illegally discriminate against minorites. Since, Adjusted R-squared of model without minor has 71.91% total variation whereas the adjusted R-squared with minor had 74.16 of total variation. So, we conclude that there was acutally discriminiation upon minor people. 

## Apendix

*Visualization*
```{r, echo=FALSE}
project <- read_csv("civil_rights.csv")
summary(project)
```

*Scatterplot of plan vs fires*
```{r}
ggplot(mapping = aes(x = fires, y = plan), data = project)+
  geom_point(aes(color = location)) + geom_smooth(method = "lm")
#pretty linear relationship. no need to transform for this relationship

# looking basic model
fit2 <- lm(plan ~ fires, data = project)
summary(fit2)
plot(fit2, which = 1)

#histogram of residuals
hist(fit2$residuals)
```

*ScatterPlot of plan vs income*
```{r}
ggplot(mapping = aes(x = income, y = plan), data = project)+
  geom_point(aes(color = location)) + geom_smooth(method = "lm")

# looking basic model
fit3 <- lm(plan ~ income, data = project)
summary(fit3)
plot(fit3, which =1)

#histogram of residuals
hist(fit3$residuals)
```

*ScatterPlot of plan vs thefts*
```{r}
ggplot(mapping = aes(x = thefts, y = plan), data = project)+
  geom_point(aes(color = location)) + geom_smooth(method = "lm")

# looking basic model
fit4 <- lm(plan ~ thefts, data = project)
summary(fit4)
plot(fit4, which =1)

#histogram of residuals
hist(fit4$residuals)
```

*ScatterPlot of plan vs const*
```{r}
ggplot(mapping = aes(x = const, y = plan), data = project)+
  geom_point(aes(color = location)) + geom_smooth(method = "lm")

# looking basic model
fit5 <- lm(plan ~ const, data = project)
summary(fit5)
plot(fit5, which =1)

#histogram of residuals
hist(fit5$residuals)
```

## Removing the outlier
```{r}
project_new %>% filter(zip == '60610' | zip == '60611')
project_new_2 <- project_new %>% filter(zip != "60610" & zip != "60611")
print(project_new_2)
```


## Refining the model
```{r}
project_new <- project %>% 
  mutate(leverage = hatvalues(new_model),
         studres = studres(new_model),
         cooks = cooks.distance(new_model))
```

```{r, echo = FALSE}
## checking Cp Plot 
leaps_1 <- regsubsets(plan ~ fires+const+thefts+income+location, data = project_new_2)
subsets(leaps_1, statistic ='cp', main='Cp plot for all subsets regression',legend = FALSE)
#from Cp plot, we got fires, thefts and income are best selection variables. 
```

```{r}
## applying forward selection and backward elimination on model_new_2
# Forward Selection
new_model_3 <- lm(plan ~ 1, data = project_new_2)
stepAIC(new_model_3, scope = list(upper = new_model_2, lower = ~ 1), direction = "forward", trace = FALSE)$anova

# Backward Elimination
stepAIC(new_model_2, direction = "backward", trace = FALSE)$anova

# Stepwise Regression
stepAIC(new_model_2, direction = "both", trace = FALSE)$anova

#All of the output show that $plan = fires + thefts + income$ is the best model, which is consistent with the suggestions given by our previous Cp graph.Thus, we consider use this as our final model.
```

*Applying forward selection and backward elimination on complete_model*
```{r}
# Forward Selection
new_model_4 <- lm(plan ~ 1, data = project_new_2)
stepAIC(new_model_4, scope = list(upper = complete_model, lower = ~ 1), direction = "forward", trace = FALSE)$anova

# Backward Elimination
stepAIC(complete_model, direction = "backward", trace = FALSE)$anova

# Stepwise Regression
stepAIC(complete_model, direction = "both", trace = FALSE)$anova

#All of the output show that $plan = minor + fires + thefts$ is the best model, which is consistent with the suggestions given by our previous Cp graph.Thus, we consider use this as our final model.
```

*Cp plot of complete_model*
```{r, echo = FALSE}
leaps_2 <- regsubsets(plan ~ minor+fires+thefts+income, data = project_new_2)
subsets(leaps_2, statistic ='cp', main='Cp plot for all subsets regression',legend = FALSE)
```

*BIC plot*
```{r}
models_2 <- regsubsets(plan ~ minor+fires+thefts+income, nbest = 3, data = project_new_2)
plot(models_2)
```

